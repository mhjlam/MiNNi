\documentclass[a4paper]{article}
\usepackage[a4paper, left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{amsmath, amssymb, bm, enumitem, graphicx}

\allowdisplaybreaks

\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\euler}{\mathrm{e}}
\newcommand{\matr}[1]{\mathbf{#1}}
\newcommand{\vect}[1]{\bm{#1}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\mul}{\text{mul}}
\newcommand{\summ}{\text{sum}}
\newcommand{\relu}{\text{ReLU}}

\newcommand{\yij}{\vect{y}_{i,j}}
\newcommand{\yhatij}{\vect{\hat{y}}_{i,j}}

\newcommand*\diff{\mathop{}\!\mathrm{d}}

\DeclareEmphSequence{\bfseries\slshape}
\setlength\parindent{0pt}
\setlength\parskip{4pt}
\setlist{noitemsep, topsep=0pt, parsep=0pt, partopsep=0pt}

\title{\vspace{-1in}Notes}
\author{}
\date{}

\begin{document}
\maketitle
\vspace{-0.5in}

\section*{Neural Network}
Neural networks can be used for three main tasks: \emph{classification}, \emph{regression} (prediction of a scalar), clustering (ordering unstructured data into groups), and more.

A neural network has a vast number of adjustable parameters, represented by weights and biases. The goal of a neuron network is to train it to fit the data instead of just memorizing it -- this is called \emph{generalization}. Training is done by slowly adjusting weights and biases on many examples of data, and calculating the loss (error) of the output.

\emph{Overfitting} occurs when the network only learns to fit the training data but is incapable of 
relating input-output dependencies, and thus the parameters are not tweaked correctly.

\subsection*{Layers}
A neural network consists of a number of layers: the input layer, the output layer, and so-called hidden layers.

The \emph{input layer} represents the input samples, like pixel values or sensor data. This input needs to be preprocessed so that it is normalized, scaled, and in numeric form.

The \emph{output layer} represents the output value. With classification the output layer typically has as many neurons as the amount of classes, but it can also be binary classification (true or false).

Any layers between these two are called \emph{hidden layers}. A neural network is \emph{deep} when there are two or more such layers. Hidden layers consist of neurons with a vast number of tweakable parameters: the weights and biases. 

There are several layer types such as dense layers, dropout layers, convolutional layers, or recurrent layers. \emph{Dense layers} are those where each neuron in the layer is connected to each neuron in the previous and next layer. This type of layer is also called \emph{linear} or \emph{fully-connected}.

Dense layers learn feature values and the correlation between them, while \emph{convolutional} layers are trained to find and understand features like characteristics or traits. 

\subsection*{Parameters}
When training, the weights of a layer are initialized randomly to a small non-zero value, and the biases set to zero. Otherwise, in a pre-trained model the parameters are initialized to the previously determined output of that model. 

If a neuron no longer affects the model's result on any input and returns 0 on most inputs, then the neuron or even the network is essentially untrainable, or "dead". In this case, tweaking the bias to some non-zero value will lead to more favorable results. 

Layer inputs use data batches because the output of a layer should be sample-related and not neuron-related. Samples are passed further through the network, and the next layer will also expect a batch of inputs. 

\begin{itemize}
    \item $\matr{X}$: matrix of input samples. One input sample is given as $x \in \matr{X}$.
    \item $\bm{y}$: target classification labels.
    \item $\vect{\hat{y}}$: predicted classification (the values of the output layer).
    \item $\matr{\Theta}$: matrix of outputs of a hidden layer. The output of one neuron is $\theta \in \matr{\Theta}$
\end{itemize}

% SOMETHING ABOUT HOW CLASSIFICATION LABELS CAN BE SPARSE OR DENSE

\subsection*{Arrays}
An n-dimensional array is homologous if all arrays along the 'row' dimension are of equal length.

The shape of an array is determined by the dimensions, so a 3D array with 3 matrices of 2 lists of 4 elements has a shape of (3, 2, 4). Shape is an important factor for mathematical operations on arrays.

- An array is an ordered homologous container for numbers.
- A vector is a list or 1-dimensional array.
- A matrix is a 2-dimensional array. Each element of the array can be accessed using a tuple of indices as a key.
- A tensor is an n-dimensional object that can be represented as an array. 

\section*{Forward pass}
The goal of the forward pass is to run the input samples through the neural network and to calculate the loss, which is a representation of the error of the model.

A dense layer applies the following linear formula:

\begin{equation*}
    \matr{\Theta}_i = \matr{\Theta}_{i-1} \cdot \matr{W}_i + \bm{b}_i
\end{equation*}
where $\matr{\Theta}_i$ is the output matrix of a layer $i$, $\matr{W}$ is the weight matrix of this layer, and $\vect{b}$ is a vector of biases, one for each neuron in the layer.

Matrix $\matr{\Theta}_{i-1}$ can be output from a previous hidden layer, or the input sample matrix $\matr{X}$ for the first layer.

\subsection*{Activation}
After calculating the linear formula above, an activation function is applied to its output to simulate whether a neuron is activated for a certain input. Just like weights and biases, activation functions are essential parts of each neuron of a layer.

A neural network typically has two different activation functions; one used in the hidden layers, and a different one in the output layer.

If an activation function is nonlinear, it will allow neural networks with two or more hidden layers to map nonlinear functions. Two of the most commonly used activation functions, \emph{Rectified Linear Unit} (ReLU) and \emph{Softmax} are such functions.

\subsubsection*{Rectified Linear Unit}
The rectified function is a simple but powerful when using them in two (or more) successive layers because it can then approximate nonlinear functions. While simple, efficient, and nearly linear, it remains nonlinear due to the bend after 0.

\begin{equation*}
    \text{ReLU}(x) = \max(0, x)
\end{equation*}

\subsubsection*{Softmax}
The Softmax activation function is used for multiclass classification problems and is applied to the output layer. It can take in non-normalized, or uncalibrated, inputs and produce a normalized distribution of probabilities for each class that will add up to 1. The predicted class is associated with the output neuron that returned the largest confidence score.

Softmax takes as input a vector $\theta = (z_1, \ldots, z_K) \in \mathbb{R}$ of $K$ real numbers, and normalizes it into a probability distribution consisting of $K$ probabilities proportional to the exponentials of the input numbers:

\begin{equation*}
    \text{Softmax}(\theta)_i = \frac{\euler^{\theta_i}}{\sum_{k=1}^K \euler^{\theta_k}} \text{ for } i = 1, \ldots, K.
\end{equation*}

Exponentiation leads to non-negative values for each of the inputs. Due to the monotonic nature of the exponential function, higher input values also give higher outputs, so the predicted class will not change after applying exponentiation. Converting these numbers to a probability distribution that describe a vector of confidences is done via normalization in the denominator.

Softmax also includes a subtraction of the largest of the inputs before doing the exponentiation step. This mitigates two pervasive challenges with neural networks: dead neurons and exploding values, which render a network useless over time. The exponential function in the Softmax activation is actually one of the sources of exploding values. By subtracting the highest input 
value from all inputs, the maximum value will be $0$, and thus the exponentiation will only produce 
outputs between $0$ and $1$.

\subsection*{Loss}
A \emph{loss function} or \emph{cost function} quantifies the error of a model and the objective is to reduce it to $0$ as closely as possible. By calculating loss, we strive to increase correct confidence and decrease misplaced confidence.

Neural networks that do regression have a loss function that calculate the mean squared error.

\subsubsection*{Categorical Cross-Entropy}
For classification, the output of the network is a probability distribution, and a useful function when training a classification problem with classes is \emph{categorical cross-entropy}. It compares a ground-truth probability vector $\vect{y}$ and some predicted distribution $\vect{\hat{y}}$. It is one of the most commonly used loss functions with a softmax activation on the output layer.

The formula for calculating the categorical cross-entropy of $\vect{y}$ (actual/desired distribution) and $\vect{\hat{y}}$ (predicted distribution) is:

\begin{equation*}
    L_i = -\sum_{j} \vect{y}_{i,j} \log({\vect{\hat{y}}})
\end{equation*}

Where $L_i$ denotes sample loss value, $i$ is the $i$-th sample in the set, $j$ is the label/output index, $\vect{y}$ denotes the target values, and $\vect{\hat{y}}$ denotes the predicted values.

By encoding the target values $\vect{y}_{i,j}$ as one-hot encoded vectors, this equation can be simplified. One-hot means that one of the values in a vector is "hot" with a value of 1, and the rest are "cold" with values of 0. As a result, the summation over the targets $\vect{\hat{y}}$ zeroes out to just one multiplication by $1$, simplifying the equation to:

\begin{equation*}
    L_i = -\log(\vect{\hat{y}}_{i,k})
\end{equation*}

Where $L_i$ denotes sample loss value, $i$ is the $i$-th sample in a set, $k$ is the index of the target label (ground-true label), $\vect{\hat{y}}$ denotes the predicted values.

There is a problem with the categorical cross-entropy when a confidence value equals $0$; this will 
lead to the invalid calculation of $\log(0)$, which is undefined. To prevent loss from being exactly $0$ or negative, the value is clipped from both sides by the same infinitesimally small number.

\subsubsection*{Accuracy}
While loss is a useful metric for optimizing a model, accuracy describes how often the largest 
confidence is the correct class in terms of a fraction. To compute the accuracy, the $\text{argmax}$ values from the Softmax outputs $\vect{\hat{y}}$ are compared to the target classification $\vect{y}$:

\begin{equation*}
    \text{Accuracy}(\vect{\hat{y}},\vect{y}) = \frac{1}{\|\vect{\hat{y}}\|} \sum_{i=1}^{\|\vect{\hat{y}}\|} \left( \begin{cases}
                1 & \argmax(\vect{\hat{y}_i}) = \vect{y}_i \\
                0 & \argmax(\vect{\hat{y}_i}) \not= \vect{y}_i
            \end{cases}
            \right)
\end{equation*}

In other words, it is an arithmetic mean of all the predictions $\vect{\hat{y}}_i$ that correspond to the correct classification $\vect{y}_i$ (in which the result is 1).

\pagebreak
\section*{Backpropagation}
Calculating the partial derivatives of all the functions from the loss function back through all the
layers in the neural network is called backpropagation. In this backwards pass, \emph{gradient descent} is performed by calculating derivatives of single-parameter functions and gradients of multivariate functions. The end goal is to reduce the loss metric as much as possible by optimizing the model.

A partial derivative measures how much impact a single input has on a function's output. The gradient (denoted with a $\nabla$ symbol) is a vector of the size of inputs containing all of the partial derivative solutions with respect to each of the inputs and shows the direction of the steepest ascent. By applying a negative fraction to a gradient the loss can be decreased.

In a dense layer, a single neuron in the current layer is connected to all neurons in the next 
layer. During backpropagation, each neuron from the current layer will receive a vector of partial 
derivatives for all the neurons in the next layer. Each neuron will output a gradient of the partial derivatives with respect to all of their inputs.

\subsubsection*{Rectified Linear Unit (ReLU) Derivative}

The forward pass with the ReLU function for one neuron with inputs $\matr{X}$, weights $\matr{W}$, and biases $\vect{b}$ is:

\begin{alignat*}{2}
    y &= \relu(\matr{X} \cdot \matr{W} + \vect{b}) \text{ or} \\
    y &= \relu(x_1w_1 + x_2w_2 + x_3w_3 + \vect{b})
\end{alignat*}

This equation contains three nested functions: $\max$, a summation and multiplications. The partial derivative to any input $x_i$ (or any other of the inputs) is obtained by applying the \emph{chain rule} to the nested $\max$, $\summ$, and $\mul$ functions:

\begin{alignat*}{2}
    \frac{\partial}{\partial x_i}[\relu(\summ(\mul(x_1,w_1),\mul(x_i,w_i),\mul(x_n,w_n)))] &=\\
    \frac{d\relu(\dots)}{d\summ(\dots)} \cdot \frac{\partial\summ(\dots)}{\partial\mul(x_i,w_i)} \cdot \frac{\partial\mul(x_i,w_i)}{\partial x_i} &
\end{alignat*}

Firstly, the partial derivative of the $\summ$ operation is always $1$, no matter the inputs:

\begin{alignat*}{2}
    f(x,y) = x+y ~~ \rightarrow ~~ &\frac{\partial}{\partial x}f(x,y) = 1 \\
                                   &\frac{\partial}{\partial y}f(x,y) = 1
\end{alignat*}

Secondly, the derivative for a product ($\mul$ operation) is whatever the input is being multiplied by:

\begin{alignat*}{2}
    f(x,y) = x \cdot y ~~ \rightarrow ~~ &\frac{\partial}{\partial x}f(x,y) = y \\
                                         &\frac{\partial}{\partial y}f(x,y) = x
\end{alignat*}

Thirdly, derivative of $\relu$ with respect to input $x$ is $1$ if $x > 0$, and $0$ otherwise:

\begin{equation*}{}
    \frac{\partial}{\partial\relu(x)} = \frac{\partial}{\partial x}\max(0, x) = 1 \: (x > 0)
\end{equation*}

And thus finally -- following the chain rule -- the full derivative of the $\relu$ function with respect to input $x_i$ becomes:

\begin{equation*}{}
    \frac{\partial}{\partial x_i}\relu(x_i) = \partial_v \cdot 1 \: (z > 0) \cdot w_i \text {, with } z = \vect{x} \cdot \vect{w} + b
\end{equation*}

Where $\partial_v$ is the derivative calculated in the next layer, and $z$ is the weighted sum of the inputs and the bias.

Combining all the partial derivatives above for each input $x_i \in X$ make up the gradients and can be represented as $\nabla_x$ for the inputs, $\nabla_w$ for the weights, and $\nabla_b$ for the bias:

\begin{equation*}
    \nabla_x = 
    \begin{bmatrix}
        \frac{\partial}{\partial x_1}\relu(x_1)\\
        \frac{\partial}{\partial x_i}\relu(x_i)\\
        \frac{\partial}{\partial x_n}\relu(x_n)
    \end{bmatrix}
\end{equation*}
\begin{equation*}
    \nabla_w = 
    \begin{bmatrix}
        \frac{\partial}{\partial w_1}\relu(w_1)\\
        \frac{\partial}{\partial w_i}\relu(w_i)\\
        \frac{\partial}{\partial w_n}\relu(w_n)
    \end{bmatrix}
\end{equation*}
\begin{equation*}
    \nabla_b = 
    \begin{bmatrix}
        \frac{\partial}{\partial b}\relu(b)
    \end{bmatrix}
\end{equation*}

\subsubsection*{Categorical Cross-Entropy Derivative}

To calculate the gradient we need to use the partial derivative of the full forward function, which is:

\begin{equation*}
    L_i = -\sum_{j} \vect{y}_{i,j} \log(\vect{\hat{y}}_{i,j})
\end{equation*}

Where $L_i$ denotes sample loss value, $i$ is the $i$-th sample in a set, $j$ is the label index, $\vect{y}$ are the target values, and $\vect{\hat{y}}$ are the predicted values.

Let's define the gradient equation as the partial derivative of the loss function with respect to each of its inputs and solve it using the chain rule:

\begin{alignat*}{5}
    \frac{\partial L_i}{\partial \vect{\hat{y}}_{i,j}} &= \frac{\partial}{\partial \vect{\hat{y}}_{i,j}} [-\sum_j \vect{y}_{i,j} \log(\vect{\hat{y}}_{i,j})] \\
    &= -\sum_j \vect{y}_{i,j} \cdot \frac{\partial}{\partial \vect{\hat{y}}_{i,j}} \log(\vect{\hat{y}}_{i,j}) \\
    &= -\sum_j \vect{y}_{i,j} \cdot \frac{1}{\vect{\hat{y}}_{i,j}} \cdot \frac{\partial}{\partial \vect{\hat{y}}_{i,j}} \vect{\hat{y}}_{i,j} \\
    &= -\sum_j \vect{y}_{i,j} \cdot \frac{1}{\vect{\hat{y}}_{i,j}} \cdot 1 \\
    &= -\sum_j \frac{\vect{y}_{i,j}}{\vect{\hat{y}}_{i,j}}
\end{alignat*}

The summation can be omitted because the partial derivative is calculated with respect to the $\vect{y}$, predicted values at the $i$-th sample and given index $j$. Thus the sum is performed over a single element at $j$. The derivative of this loss function with respect to its inputs then equals the negative ground-truth vector ($\vect{y}$), divided by the vector of the predicted values ($\vect{\hat{y}}$):

\begin{equation*}
    \frac{\partial L_i}{\partial \vect{\hat{y}}_{i,j}} = -\frac{\vect{y}_{i,j}}{\vect{\hat{y}}_{i,j}}
\end{equation*}

\subsubsection*{Softmax Derivative}
The definition of the partial derivative of the Softmax function is:

\begin{equation*}
    \matr{S}_{i,j} = \frac{\euler^{\theta_{i,j}}}{\sum_{k=1}^K \euler^{\theta_{i,k}}} ~~ \rightarrow ~~ \frac{\partial \matr{S}_{i,j}}{\partial \theta_{i,m}} = \frac{\partial \frac{\euler^{\theta_{i,j}}}{\sum_{k=1}^{K} \euler^{\theta_{i,k}}}}{\partial \theta_{i,m}}
\end{equation*}

Where $\matr{S}_{i,j}$ denotes $j$-th Softmax's output of the $i$-th sample, $\theta$ is the input array which is a list of output vectors from the previous layer, $\theta_{i,j}$ is the $j$-th Softmax's input of the $i$-th sample, $K$ is the number of classifications, and $\theta_{i,m}$ is the $m$-th Softmax's input of the $i$-th sample.

The Softmax function equals the exponentiated input divided by the sum of all exponentiated inputs, so we need to exponentiate all of the values first and then divide each of them by the sum of all of them to perform the normalization. Because each input of the Softmax impacts the each of the outputs, each partial derivative of each output with respect to each input has to be calculated. 

Applying the chain rule and solving with respect to input $\theta_{i,k}$ yields:

\begin{alignat*}{3}
    \frac{\partial \matr{S}_{i,j}}{\partial \theta_{i,m}} &= \frac{\partial \frac{\euler^{\theta_{i,j}}}{\sum_{k=1}^{K} \euler^{\theta_{i,k}}}}{\partial \theta_{i,m}} \\
    &= \frac{\frac{\partial}{\partial \theta_{i,m}} \euler^{\theta_{i,j}} \cdot \sum_{k=1}^{K} \euler^{\theta_{i,k}} - \euler^{\theta_{i,j}} \cdot \frac{\partial}{\partial \theta_{i,m}} \sum_{k=1}^{K} \euler^{\theta_{i,k}}}{[\sum_{k=1}^{K} \euler^{\theta_{i,k}}]^2} \\
    &= \frac{\frac{\partial}{\partial \theta_{i,m}} \euler^{\theta_{i,j}} \cdot \sum_{k=1}^{K} \euler^{\theta_{i,k}} - \euler^{\theta_{i,j}} \cdot \frac{\partial}{\partial \theta_{i,m}} \sum_{k=1}^{K} \euler^{\theta_{i,k}}}{[\sum_{k=1}^{K} \euler^{\theta_{i,k}}]^2} \\
\end{alignat*}

Now there are two possibilities for the partial derivative on the left side of the subtraction operation that lead to different results: one where $j=m$ and one where $j\not=m$.

In the case of $j=m$:

\begin{alignat*}{5}
    \frac{\partial \matr{S}_{i,j}}{\partial \theta_{i,m}} &= 
    \frac{\euler^{\theta_{i,j}} \cdot \sum_{k=1}^{K} \euler^{\theta_{i,k}} - \euler^{\theta_{i,j}} \cdot \euler^{\theta_{i,m}}}{[\sum_{k=1}^{K} \euler^{\theta_{i,k}}]^2} \\
    &= \frac{\euler^{\theta_{i,j}} \cdot (\sum_{k=1}^{K} \euler^{\theta_{i,k}} - \euler^{\theta_{i,m}})}{\sum_{k=1}^{K} \euler^{\theta_{i,k}} \cdot \sum_{k=1}^{K} \euler^{\theta_{i,k}}} \\
    &= \frac{\euler^{\theta_{i,j}}}{\sum_{k=1}^{K} \euler^{\theta_{i,k}}} \cdot \frac{\sum_{k=1}^{K} \euler^{\theta_{i,k}} - \euler^{\theta_{i,m}}}{\sum_{k=1}^{K} \euler^{\theta_{i,k}}} \\
    &= \frac{\euler^{\theta_{i,j}}}{\sum_{k=1}^{K} \euler^{\theta_{i,k}}} \cdot \frac{\sum_{k=1}^{K} \euler^{\theta_{i,k}}}{\sum_{k=1}^{K} \euler^{\theta_{i,k}}} - \frac{\euler^{\theta_{i,m}}}{\sum_{k=1}^{K} \euler^{\theta_{i,k}}}
\end{alignat*}

Looking closely, both the left part and right part of the equation are just the definitions of the Softmax function itself:

\begin{equation*}
    \frac{\partial \matr{S}_{i,j}}{\partial \theta_{i,m}} = \matr{S}_{i,j} \cdot (1 - \matr{S}_{i,m}) ~~ \text{ when } j=m
\end{equation*}

In the case of $j\not=m$:

\begin{alignat*}{5}
    \frac{\partial \matr{S}_{i,j}}{\partial \theta_{i,m}} &= \frac{\frac{\partial}{\partial \theta_{i,m}} \euler^{\theta_{i,j}} \cdot \sum_{k=1}^{K} \euler^{\theta_{i,k}} - \euler^{\theta_{i,j}} \cdot \frac{\partial}{\partial \theta_{i,m}} \sum_{k=1}^{K} \euler^{\theta_{i,k}}}{[\sum_{k=1}^{K} \euler^{\theta_{i,k}}]^2} \\
    &= \frac{0 \cdot \sum_{k=1}^{K}\euler^{\theta_{i,k}} - \euler^{{z}_{i,j}} \cdot \euler^{\theta_{i,m}}}{[\sum_{k=1}^{K} \euler^{\theta_{i,k}}]^2} \\
    &= \frac{- \euler^{{z}_{i,j}} \cdot \euler^{\theta_{i,m}}}{\sum_{k=1}^{K} \euler^{\theta_{i,k}} \cdot \sum_{k=1}^{K} \euler^{\theta_{i,k}}} \\
    &= -\frac{\euler^{\theta_{i,j}}}{\sum_{k=1}^{K} \euler^{\theta_{i,k}}} \cdot \frac{\euler^{\theta_{i,m}}}{\sum_{k=1}^{K} \euler^{\theta_{i,k}}}
\end{alignat*}

For this case the Softmax function can also be identified in both the left and right parts of the equation:

\begin{equation*}
    \frac{\partial \matr{S}_{i,j}}{\partial \theta_{i,m}} = -\matr{S}_{i,j} \cdot \matr{S}_{i,m} ~~ \text{ when } j\not=m
\end{equation*}

Thus, the two derivatives calculated above can be represented as:

\begin{equation*}
    \frac{\partial\matr{S}_{i,j}}{\partial\theta_{i,m}} = 
    \begin{cases}
        \matr{S}_{i,j} \cdot (1 - \matr{S}_{i,m}) & ~~ \text{ when } ~ j = m \\
        \matr{S}_{i,j} \cdot (0 - \matr{S}_{i,m}) & ~~ \text{ when } ~ j \not= m
      \end{cases}       
\end{equation*}

By applying the \emph{Kronecker delta function} the equation simplifies even further:

\begin{equation*}
    \matr{S}_{i,j}\delta_{j,k} - \matr{S}_{i,j}\matr{S}_{i,k} ~~ \text{where} ~~ \delta_{i,j} =
    \begin{cases}
        1 & i=j\\
        0 & i\not=j
    \end{cases}
\end{equation*}

The result of this is a Jacobian matrix; an array of partial derivatives in all of the of both input vectors. This is then done for every output of the Softmax function with respect to each input separately (a batch of samples), and thus the result becomes a list of Jacobian matrices, or a 3D matrix. This result then needs to be condensed to a single partial derivative value by taking each row of the Jacobian matrix and multiplying it with the corresponding value in the loss function's gradient.

\subsubsection*{Softmax Cross-Entropy}
By combining the \emph{Softmax activation} and the \emph{Categorical Cross-Entropy loss} functions, their derivative can be greatly simplified, leading to a significant performance boost. The simplification is possible due to the the inputs of the cross-entropy function being equal to the outputs of the Softmax function, and using the properties of the one-hot encoded vector to reduce a summation to a scalar value.

The chain rule can be applied to calculate the partial derivative of the Categorical Cross-Entropy loss function with respect to the Softmax's inputs:

\begin{alignat*}{10}
    \frac{\partial L_i}{\partial \theta_{i,m}} &= \frac{\partial L_i}{\partial \yhatij} \cdot \frac{\partial \matr{S}_{i,j}}{\partial \theta_{i,m}} = \frac{\partial L_i}{\partial \yhatij} \cdot \frac{\partial \yhatij}{\partial \theta_{i,m}} \\
    &= -\sum_{j}\frac{\yij}{\yhatij} \cdot \frac{\partial \yhatij}{\partial \theta_{i,m}} \\
    &= -\frac{\vect{y}_{i,m}}{\vect{\hat{y}}_{i,m}} \cdot \frac{\partial \vect{\hat{y}}_{i,m}}{\partial \theta_{i,m}} - \sum_{j\not=m}\frac{\yij}{\yhatij} \cdot \frac{\partial\yhatij}{\partial\theta_{i,m}} \\
    &= -\frac{\vect{y}_{i,m}}{\vect{\hat{y}}_{i,m}} \cdot \yhatij \cdot (1 - \vect{\hat{y}}_{i,m}) - \sum_{j\not=m}\frac{\yij}{\yhatij}(-\yij\vect{y}_{i,m}) \\
    &= -\vect{y}_{i,m} \cdot (1 - \vect{\hat{y}}_{i,m}) + \sum_{j\not=m}\yij\vect{\hat{y}}_{i,m} \\
    &= -\vect{y}_{i,m} + \vect{y}_{i,m} \vect{\hat{y}}_{i,m} + \sum_{j\not=m}\yij\vect{\hat{y}}_{i,m} \\
    &= -\vect{y}_{i,m} + \sum_{j}\yij\vect{\hat{y}}_{i,m} \\
    &= -\vect{y}_{i,m} + \vect{\hat{y}}_{i,m} = \vect{\hat{y}}_{i,m} - \vect{y}_{i,m}
\end{alignat*}

After applying the chain rule to the partial derivatives of cross-entropy loss function $L_i$ and of the Softmax function $\matr{S}_{i,j}$, the whole equation simplifies significantly to the subtraction of the predicted labels $\vect{\hat{y}}$ and the ground-truth labels $\vect{y}$.

\section*{Optimizers}
During backpropagation the calculated gradient points to the current steepest loss ascent, and taking the negative of the gradient vector flips it toward the current steepest descent. The goal of an optimizer is to adjust a model's parameters to approach the \emph{global minimum} of the loss gradient. When there are millions of dimensions (parameters) in a function, gradient descent is the best known way to search for a global minimum.

As long as the loss is not very close to 0 the model stopped learning and it has become stuck in a \emph{local minimum}. However, if a loss of 0 is reached, this is a reason to be suspicious, because it usually means that the model has been overfitted to the training data. Conversely, \emph{generalization} is the ability of the model to correctly predict unseen data, or to approximate the "trend" of the data closely.

An \emph{epoch} is a full cycle of a forward then backward pass and then optimization through all of the training data. A neural network will usually be trained for multiple epochs.

Most optimizers are variants of \emph{Stochastic Gradient Descent} (SGD).

\subsection*{Stochastic Gradient Descent}
Stochastic Gradient Descent historically refers to an optimizer that fits a single sample at a time, but modern variants is seen as one that assumes a batch of data, whether that batch happens to be a single sample, every sample in a dataset, or some subset of the full dataset at a time.

Stochastic Gradient Descent uses a factor called the \emph{learning rate} that is applied to the gradients before subtracting them from the weight and bias parameters. The learning rate is a so-called \emph{hyper-parameter}, a global parameter that is configured for the optimizer.

\emph{Gradient descent} is the simplest optimization algorithm which computes gradients of loss function with respect to model parameters and updates them by using the following formula:

\begin{equation*}
    \theta_t = \theta_{t-1} - \eta \, \nabla_\theta
\end{equation*}

Where $\theta$ is a parameter (weight or bias), $\nabla_\theta$ is the gradient of $\theta$, $\eta$ is the learning rate, and $t$ is the step/iteration/epoch index. 

\subsubsection*{Learning Rate}
The learning rate $\eta$ is a scalar value that modifies the influence of the gradient descent. By adjusting this parameter correctly, the global minimum can be closely approximated. 

By taking small steps in the direction of the gradient we can ensure that the steepest descent is followed. However, steps that are too small can cause \emph{learning stagnation}, and thus if the learning rate is too small, the optimizer may get stuck in a local minimum. Having a learning rate that is too high will lead to an unstable model where it may not be able to find the global minimum, or further epochs may cause a \emph{gradient explosion}. This is a situation where parameter updates cause the loss and gradient to increase instead of decrease.

Choosing the right hyper-parameters is critical and challenging. It is usually best to start with the optimizer defaults, perform a few steps, and observe the training process after tuning. In most cases it is beneficial start with a higher learning rate and decrease it during training using a \emph{learning rate decay}.

\subsubsection*{Learning Rate Decay}
One method to add decay is to decrease the learning rate manually or logically in response to the loss across epochs, as when it begins to level out or starts jumping over large deltas. 

Another way is to implement a \emph{decay rate} which steadily decays the learning rate per batch or epoch. A step-wise decay rate is also referred to as \emph{1/t decaying} or \emph{exponential decaying}. 

The learning rate decay is defined as the reciprocal of the step count fraction:

\begin{equation*}
    \eta = \eta_0 \, \frac{1}{1 + \beta \, t}
\end{equation*}

Where $\eta$ is the learning rate for the current step $t$, $\eta_0$ is the initial learning rate, and $\beta$ is the decay rate. The formula will make the learning rate smaller with time, because the denominator will get higher with each step.

If the learning rate decays too quickly and becomes too small, the model gets trapped in a local minimum. Therefore, the decay is usually a small number like $0.001$ or $0.00001$.

\subsubsection*{Stochastic Gradient Descent with Momentum}
Adding \emph{momentum} to an optimizer adds "inertia" to the steps taken towards the global minimum and can mitigate learning stagnation and gradient explosion. Momentum retains the direction of the gradient from a previous step and uses it to influence the next update's direction.

The momentum $\gamma$ represents the fraction of the previous parameter update to retain, and subtracting the actual gradient, multiplied by the learning rate, from it:

\begin{align*}
    v_t &= \gamma \, v_{t-1} + \eta \, \nabla\theta_t \\
    \theta_t &= \theta_{t-1} - v_t
\end{align*}

Where $\theta_t$ are the weights for the current step $t$, $\gamma \in \{0,1\}$ is the momentum, $\theta_{t-1}$ is are the weights of the previous step, $\eta$ is the learning rate for step $t$, and $\nabla\theta_t$ is the gradient, or derivative of loss with respect to weight. In practice, this equation is similarly applied to determine the adjusted biases.

The Momentum optimizer is usually one of two main initial choices, the other being \emph{Adam}. 

\subsubsection*{AdaGrad}
\emph{Adaptive Gradient} or AdaGrad uses an adaptive per-parameter learning rate rather than a global one. 

The concept is that some parameters (weights/biases) matter more to the loss than others, and thus some parameters should be updated more significantly than others in each epoch. In AdaGrad the parameters will have a different learning rate based on whether the features are \emph{dense} or \emph{sparse}. That is, if gradients corresponding to a certain parameter are large, then the respective learning rate will be small, and conversely, for smaller gradients the learning rate will be bigger. AdaGrad can deal with vanishing and exploding gradient problems this way.

The \emph{sum of squared gradients} captures how much a parameter has been updated already. Instead of keeping track of the sum of gradients like momentum, AdaGrad keeps track of the \emph{sum of squared gradients} in a \emph{cache} to adapt the gradient in a certain direction.

During the update step, AdaGrad scales the learning rate by dividing it by the square root of the accumulated gradients:

\begin{align*}
    v_t &= v_{t-1} + {\nabla\theta_t}^2 \\
    \theta_t &= \theta_{t-1} - \frac{\eta}{\sqrt{v_t} + \epsilon} \nabla\theta_t
    %\theta_t &= \theta_{t-1} - \frac{\eta}{\sqrt{\sum_{\tau=1}^{t}({\nabla\theta_\tau}^{2}}) + \epsilon} \nabla \theta_t
\end{align*}

Where $v_t$ is the sum of squared gradients at step $t$, $\theta_t$ are the weights at step $t$, $\nabla\theta_t$ are the gradients from all previous steps, $\eta$ is the initial learning rate, and $\epsilon$ is a small positive term to prevent possible division by zero.

The advantage of AdaGrad is that there is no need to manually adjust the learning rate during training, but the learning rate constantly decays over time and therefore converges slowly during later steps.

\subsubsection*{RMSProp}
RMSProp stands for \emph{Root Mean Square Propagation}, another adaptive optimizer that calculates a learning rate per parameter. It was designed as an improvement over AdaGrad's issue of learning rate decay and its formula is similar:

\begin{align*}
    v_t &= \rho \, v_{t-1} + (1-\rho) {\nabla\theta_t}^2 \\
    \theta_t &= \theta_{t-1} - \frac{\eta}{\sqrt{v_t} + \epsilon} \, \nabla\theta_t
\end{align*}

Where the new hyper-parameter $\rho$ is the cache memory decay rate. The value of this parameter can be set much lower than a standard learning rate because RMSProp carries over a lot of momentum of gradient and small gradient updates are enough to keep it going. Therefore, the recommended default is $\rho = 0.001$.

%v_t = rho * v_t + (1-rho) * dw**2
%w = w - (lr * dw / (sqrt(v_t) + epsilon))

Instead of storing a cumulated sum of squared gradients, the \emph{exponentially moving average} $v_t$ is calculated for squared gradients ${\nabla\theta}^2$. RMSProp converges faster than AdaGrad because the exponentially moving average puts more emphasis on recent gradient values rather than equally distributing importance between all gradients. Additionally, the learning rate does not always decay with the increase of iterations. 

\subsubsection*{Adam}
Adam is short for \emph{Adaptive Moment Estimation} and combines RMSProp with Momentum. In addition to storing an exponentially decaying average of past squared gradients $v_t$ like RMSprop, it also keeps an exponentially decaying average of past gradients $m_t$, similar to momentum:

\begin{alignat*}{2}
    v_t = \beta_1 v_{t-1} &+ (1 - \beta_1) \nabla\theta_t \\
    s_t = \beta_2 s_{t-1} &+ (1 - \beta_2) {\nabla\theta_t}^2
\end{alignat*}

Where $\beta_1$ and $\beta_2$ are the decay rates that are recommended to be set at $\beta_1 = 0.9$ and $\beta_2 = 0.999$. Moving averages $v_t$ and $s_t$ are estimates of the \emph{first moment} (the mean) and the \emph{second moment} (the uncentered variance) of the gradients respectively, hence the name of this optimizer.

Initially, and the decay rates are small (close to $1$), the moments are biased towards zero. Those biases can be mitigated by computing the \emph{bias-corrected first and second moment estimates}:

\begin{align*}
    \hat{v}_t = \frac{v_t}{1-\beta_1^t} \\
    \hat{s}_t = \frac{s_t}{1-\beta_2^t}
\end{align*}

And then use these estimates to update the parameters according to Adam's update rule:

\begin{align*}
    \theta_t = \theta_{t-1} - \frac{\eta}{\sqrt{\hat{s}_t} + \epsilon} \, \hat{v}_t
\end{align*}

%v_t = (beta_1*v_t + (1-beta_1)*dw) / (1-beta_1^t)
%s_t = (beta_2*s_t + (1-beta_2)*dw^2) / (1-beta_2^t)
%w = w - (lr * v_t / (sqrt(s_t) + epsilon))

\subsubsection*{Hyper-parameters}
In general it is usually a good idea to try the Adam optimizer, but sometimes simple SGD or Momentum performs better than Adam. A general starting learning rate for SGD is $1.0$, with a decay down to $0.1$. For Adam, a good starting learning rate is $0.001$, decaying down to $0.0001$. Keep in mind that different problems may require different values, but these are decent starting values.

\pagebreak
\section*{Validation}
The expectation of a well-trained neural network is that it can learn a representation of a clear set of rules and use this representation to predict classes of additional generated data. 

Unfortunately, the complexity of neural networks is both a strength and a weakness. Having a massive amount of tunable parameters means that they are exceptional at closely fitting to the training data. An overfit model is exceptional at predicting the data that it has already seen, but does significantly worse on new, similar data.

\subsection*{Avoiding Overfitting}
To avoid overfitting, it is essential to have both \emph{training data} and \emph{testing data} as separate sets. This way, some data is reserved and withheld from the training data to test the model's effectiveness. 

Overfitting can be recognized when testing data results begin to significantly diverge in trend from training data. Having a loss difference of 10\% or more between the training and testing phases is a common sign of serious overfitting. In general, the goal is to have the testing loss identical to the training loss, even if that means higher loss and lower accuracy on the training data. Similar performance on both datasets means that model has generalized.

Divergence of the training and testing data can often take a long time. One general rule to follow when selecting initial model hyperparameters is to find the smallest model possible that still learns. With fewer neurons it's easier for a neural network to generalize compared to memorizing the data. The process of trying different model settings is called \emph{hyperparameter tuning}.

Other ways to avoid overfitting are \emph{regularization} or using a so-called \emph{dropout layer}. 

\subsection*{Validating}
Hyperparameter tuning can be performed using another dataset called \emph{validation data}. When the training data is large, a part of it can separated and used for validation purposes.

If the training data is sparse, there are two options. One is to temporarily split the training data into a smaller training dataset and a validation dataset for hyperparameter tuning. Then train the model on all of the training data when the hyperparameters are finalized.

The second is \emph{$k$-fold cross-validation}: split the training dataset into $k$ amount of parts and train the model on all except one of those parts and validate it on the remaining part. Then repeat this process for all of the parts, choosing a different one used for validation each time.

When validating it is common to loop over different hyperparameter sets while running the training phase multiple times, applying different settings each run, and reviewing the results to choose the best set of hyperparameters.

\subsection*{Preprocessing}
\emph{Preprocessing} refers to the technique of preparing the training dataset (or samples) for use in the model. This includes but is not limited to: standardization, scaling, variance scaling, mean removal, non-linear transformations, and scaling to outliers. Neural networks usually perform best on data consisting of numbers in a range of $[0,1]$ or $[-1,1]$. For example for pixel values in range $[0,255]$ they would be scaled to either of these ranges.

There are two main reasons that these ranges are preferred:

\begin{itemize}
    \item Centering data on zero helps with model training because it will attenuate weight biasing in some direction. This is because many activation functions actually behave quite well within this described range. 
    \item In multiplication operations with numbers in the $[-1,1]$ range the result becomes a small value, a fraction. Big numbers on the other hand may cause instability or numerical overflow.
\end{itemize}

Not all values have to strictly be only in the $[-1,1]$ range because the model will perform well with data slightly outside of this range or with a few major outliers.

All datasets should be transformed in the same way, be it the training, validation, or testing data. This transformer should be saved along with the model and datasets and also applied during \emph{prediction}.

\emph{Data augmentation} can be used in cases where the number of training samples is low. The idea is to copy the existing samples, make a slight modification to it that still retains all the required features that the model should be able to handle, and use it as an additional sample. An example would be a photo that can be safely rotated, cropped, or scaled.

The amount of required samples to train a model may vary greatly, and depends on the data complexity and model size. Usually a few thousand (or tens of thousands) per class or feature will be necessary.

\section*{Regularization}
\emph{Regularization methods} are used to reduce generalization error. 

\subsection*{L1 and L2 Regularization}
In \emph{L1} and \emph{L2} regularization a \emph{penalty} value is calculated and added to the loss to penalize the model for large weights and biases. Large weights might indicate that a neuron is attempting to memorize a feature but in general it is better to have many neurons contributing to a model's output.

L1 regularization's penalty is the sum of all absolute values for the weights and biases:
\begin{equation*}
    L_{1_\theta} = \lambda\sum_m\abs{\theta_m}
\end{equation*}

Where $\lambda$ is a value representing the impact that this regularization carries and is usually a small value like $0.0005$, and parameter $m$ is an arbitrary iterator over all of the parameters (weights or biases) in a model.

L2 regularization's penalty is the sum of the squared weights and biases:
\begin{equation*}
    L_{2_\theta} = \lambda\sum_m\theta_m^2
\end{equation*}

L1 defines a linear penalty because the loss returned by this function is directly proportional to parameter values, whereas L2 defines a non-linear approach that penalizes larger parameters more than smaller ones. Because L1 penalizes small weights more than L2, it is rarely used alone and usually combined with L2, or not used at all. Note that regularization functions of this type drive the sum of parameters towards 0, which can help in cases of exploding gradients.

The overall loss is now defined as:
\begin{equation*}
    Q_i = L_i + L_{1_\theta} + L_{2_\theta}
\end{equation*}

\subsubsection*{Backward pass}
For the backward pass we need to know the derivatives of $L_{2_\theta}$ and $L_{1_\theta}$.

\begin{alignat*}{3}
    \frac{\partial L_{2_\theta}}{\partial\theta_m} &= \frac{\partial}{\partial\theta_m}\left[\lambda\sum_m\theta_m^2\right] \\
    &= \lambda\frac{\partial}{\partial\theta_m}\,\theta_m^2 \\
    &= 2\lambda\theta_m
\end{alignat*}

For $L_{1_\theta}$ the derivative of the absolute value piecewise function must be computed, which effetively multiplies a value by $-1$ if it is less than $0$, and by $1$ otherwise:

\begin{alignat*}{3}
    \frac{\partial L_{1_\theta}}{\partial\theta_m} &= \frac{\partial}{\partial\theta_m}\lambda\sum_m\abs{\theta_m} \\
    &= \lambda\frac{\partial}{\partial\theta_m}\abs{\theta_m} \\
    &= \lambda
    \begin{cases}
\hphantom{-}1 & \theta_m > 0 \\
          - 1 & \theta_m < 0 \\
      \end{cases}       
\end{alignat*}

Regularization terms are usually only added to the hidden layers and not the output layer. In general, regularization allows to create much larger models (with many more neurons) without fear of overfitting.

\subsection*{Dropout}
Another option for regularization is adding a \emph{Dropout Layer}. Dropout works by randomly disabling neurons at a given rate during every forward pass, forcing the network to learn how to make accurate predictions with only a random part of neurons remaining. It forces the model to use more neurons for the same purpose, resulting in a higher chance of learning the underlying function, and becoming much less dependent on any specific arrangement of neurons.

Dropout mitigates \emph{co-adoption} and overfitting, and becomes more robust against \emph{noise} and other perturbations in the training data. Co-adoption happens when neurons depend on the output values of other neurons and do not learn the underlying function by themselves. As such, dropout is a method to disable, or filter out, certain neurons in an attempt to regularize and improve the model's ability to generalize.

A dropout layer "turns off" neurons with a filter with numbers drawn from a \emph{Bernoulli distribution}. This is a binary probability distribution which gives a value of $1$ with a probability of $p$ and a value of $0$ with a probability of $q$. A random draw $r_i$ from this distribution is given as:

\begin{alignat*}{3}
    r_i \sim \text{Bernoulli}(p) &\text{ where } \\
    P(r_i=1) &= p \\
    P(r_i=0) &= q = 1 - P(r_i=1)
\end{alignat*}

Where $P(r_i=x)$ represents the probability that $r_i$ equals $x$.

The hyperparameter for a dropout layer is the \emph{dropout rate} $q$; the value in range $\{0..1\}$ for the percentage of neurons to disable in a given layer. A Bernoulli distribution is a special case of a Binomial probability distribution $Pr(k,n,p)$ with $n=1$, and is given as:

\begin{equation*}
    Pr(k,n,p) = \binom{n}{k} \, p^k \, (1-p)^{n-k}
\end{equation*}

Where $n$ is the number of trials (or experiments), $k$ is the number of successes, and $p$ the success probability for each trial. The output of a layer will be multiplied by the chosen neurons according to the probability distribution, where those that have not been selected are set to zero:

\begin{equation*}
    \theta = \theta \, \frac{Pr(1, 1-p, \|\theta\|)}{(1-p)}
\end{equation*}

Where $\theta$ are the output parameters of a layer, and $\|\theta\|$ is the size of the this array. The division by $(1-p)$ scales the data after a dropout, to mimic the mean of the sum when all of the neurons output their values.

\subsubsection*{Backward Pass}
For the backward pass we need the partial derivative of the dropoout operations. When the value of $r_i$ equals 1, then its function and derivative become the neuron's output $\theta$, compensated for the loss value by $1-p$, where $p$ is the dropout rate:

\begin{equation*}
    \frac{\partial}{\partial\theta}\left[\frac{z}{1-p}\right] = \frac{1}{1-p} \cdot \frac{\partial}{\partial\theta}\theta = \frac{1}{1-p}
\end{equation*}

And when $r_i = 0$ the dropout filter returns $0$, and thus the derivative is also $0$. Both cases combined yields:

\begin{equation*}
    Dr_i = \begin{cases}
        \frac{\theta_i}{1-p} & r_i = 1 \\
        0 & r_i = 0
    \end{cases} \rightarrow \frac{\partial}{\partial\theta_i}Dr_i = \begin{cases}
        \frac{1}{1-p} & r_i = 1 \\
        0 & r_i = 0
    \end{cases} = \frac{r_i}{1-p}
\end{equation*}

Where $Dr_i$ is the dropout for given input at index $i$.

\section*{Binary Logistic Regression}
\emph{Binary logistic regression} is a model with an output layer where each neuron separately represents two classes: 0 for one of them, and 1 for the other. This single neuron could represent any combination of two classes, like two distinct classifications or a true/false representation, and there could be many of these output neurons.

In a \emph{regressor} type of algorithm like binary logistic regression, the \emph{sigmoid} activation function is used for the output layer rather than softmax, and \emph{binary cross-entropy} for calculating loss instead of \emph{categorical cross-entropy}.

\subsubsection*{Sigmoid Activation Function}
The \emph{Sigmoid} or \emph{logistic} function is one of the classic granular activation functions used in neural networks. It is used with regressors because it scales a range of outputs from negative to positive infinity to be between $0$ and $1$, and approaches both maximum and minimum values exponentially fast. Its equation is:

\begin{equation*}
    \sigma_{i,j} = \frac{1}{1 + \euler^{-\theta_{i,j}}}
\end{equation*}

Where $\theta_{i,j}$, given indices, represents a singular output value of the layer that this activation function takes as input. Index $i$ is the current sample, and index $j$ is the current output in sample $\theta_i$. 

The derivative of the Sigmoid function is needed for the activation function's backwards step. Defining it with respect to its input yields:

\begin{align*}
    \sigma'_{i,j} &= \frac{\diff}{\diff\theta_{i,j}}\left[\frac{1}{1+\euler^{\theta_{i,j}}}\right] \\
    &= \frac{\diff}{\diff\theta_{i,j}}(1+\euler^{-\theta_{i,j}})^{-1} \\
    &= -(1+\euler^{-\theta_{i,j}})^{-2} \cdot \left( \frac{\diff}{\diff\theta_{i,j}}1+\frac{\diff}{\diff\theta_{i,j}}\theta_{i,j} \right) \\
    &= -(1+\euler^{-\theta_{i,j}})^{-2} \cdot (\euler^{-\theta_{i,j}} \cdot \frac{\diff}{\diff\theta_{i,j}} [-\theta_{i,j})] \\
    &= -(1+\euler^{-\theta_{i,j}})^{-2} \cdot -(\euler^{-\theta_{i,j}}) \\
    &= (1+\euler^{-\theta_{i,j}})^{-2} \cdot (\euler^{-\theta_{i,j}}) \\
    &= \frac{\euler^{-\theta_{i,j}}}{(1+\euler^{-\theta_{i,j}})^2} \\
    &= \frac{1}{1+\euler^{-\theta_{i,j}}} \cdot \frac{\euler^{-\theta_{i,j}}}{1+\euler^{-\theta_{i,j}}} \\
    &= \frac{1}{1+\euler^{-\theta_{i,j}}} \cdot \left( \frac{1+\euler^{-\theta_{i,j}}}{1+\euler^{-\theta_{i,j}}} - \frac{1}{1+\euler^{-\theta_{i,j}}} \right) \\
    &= \frac{1}{1+\euler^{-\theta_{i,j}}} \cdot \left( 1 - \frac{1}{1+\euler^{-\theta_{i,j}}} - \frac{1}{1+\euler^{-\theta_{i,j}}} \right) \\
    &= \sigma_{i,j} \cdot (1-\sigma_{i,j})
\end{align*}

As it turns out, the derivative of the sigmoid function equals this function multiplied by the difference of $1$ and the sigmoid function itself.

\subsection*{Binary Cross-Entropy Loss}
Similar to categorical cross-entropy loss, in \emph{binary cross-entropy} the loss can be determined using the negative log of the predictions. However, rather than only calculating this on the target class, the log-likelihoods of the correct and incorrect classes are summed for each neuron separately. Since class values are either $0$ or $1$, the incorrect class can be simplified to be a \emph{$1$-correct class} by just inverting its value. The negative log-likelihood of the correct and incorrect classes is then calculated by adding them together:

\begin{align*}
    L_{i,j} &= (y_{i,j})(-\log(\hat{y}_{i,j})) + (1-y_{i,j})(-\log(1-\hat{y}_{i,j})) \\
    &= -y_{i,j} \cdot \log(\hat{y}_{i,j}) + (1-y_{i,j}) \cdot \log(1-\hat{y}_{i,j})
\end{align*}

Where $L_{i,j}$ is calculated on a single output $j$, which is a vector of losses containing one value for each output. Because a model can contain multiple binary outputs, and each outputs its own prediction. To obtain the \emph{sample} loss we need to calculate the mean of all of these losses from a single sample:

\begin{equation*}
    L_i = \frac{1}{J}\sum_{j}L_{i,j}
\end{equation*}

Where index $i$ is the current sample, index $j$ is the current output in this sample, and $J$ is the number of outputs.

For backpropagation, we have to calculate the partial derivative of the sample loss with respect to
each input:

\begin{equation*}
    \frac{\partial L_i}{\partial\hat{y}_{i,j}} = \frac{\partial L_i}{\partial L_{i,j}} \cdot \frac{\partial L_{i,j}}{\partial\hat{y}_{i,j}}
\end{equation*}

The first term is the partial derivative of the sample loss $L_i$ with respect to a single output loss $L_{i,j}$:

\begin{equation*}
    \frac{\partial L_i}{\partial L_{i,j}} = \frac{\partial}{\partial L_{i,j}} [ \frac{1}{J}\sum_j L_{i,j} ] = \frac{1}{J} \cdot \frac{\partial}{\partial L_{i,j}} L_{i,j} = \frac{1}{J}
\end{equation*}

The second term is the \emph{atomic output loss}, a derivative of the single output loss $L_{i,j}$ with respect to the related prediction $\hat{y}_{i,j}$, and is given as:

\begin{align*}
    \frac{\partial L_{i,j}}{\partial\hat{y}_{i,j}} &= \frac{\partial}{\partial\hat{y}_{i,j}} \left[ -y_{i,j} \cdot \log(\hat{y}_{i,j}) - (1-y_{i,j}) \cdot \log(1-\hat{y}_{i,j}) \right] \\
    &= \frac{\partial}{\partial\hat{y}_{i,j}} \left[ -y_{i,j} \cdot \log(\hat{y}_{i,j}) \right] + \frac{\partial}{\partial\hat{y}_{i,j}} \left[ -(1-y_{i,j}) \cdot \log(1-\hat{y}_{i,j}) \right]\\
    &= -y_{i,j} \cdot \frac{\partial}{\partial\hat{y}_{i,j}} \log(\hat{y}_{i,j}) - (1-y_{i,j}) \cdot \frac{\partial}{\partial\hat{y}_{i,j}} \log(1-\hat{y}_{i,j}) \\
    &= -y_{i,j} \cdot \frac{1}{\hat{y}_{i,j}} \cdot \frac{\partial}{\partial\hat{y}_{i,j}}\hat{y}_{i,j} - (1-y_{i,j}) \cdot \frac{1}{1-\hat{y}_{i,j}} \cdot \frac{\partial}{\partial\hat{y}_{i,j}}[1-\hat{y}_{i,j}] \\
    &= -\frac{y_{i,j}}{\hat{y}_{i,j}} - \frac{1-y_{i,j}}{1-\hat{y}_{i,j}} \cdot \left( \frac{\partial}{\partial\hat{y}_{i,j}}1 - \frac{\partial}{\partial\hat{y}_{i,j}} \hat{y}_{i,j} \right) \\
    &= -\frac{y_{i,j}}{\hat{y}_{i,j}} - \frac{1-y_{i,j}}{1-\hat{y}_{i,j}} \cdot (0-1) \\
    &= - \left( \frac{y_{i,j}}{\hat{y}_{i,j}} - \frac{1-y_{i,j}}{1-\hat{y}_{i,j}} \right) \\
\end{align*}

And so the equation of the partial derivative of a sample loss with respect to a single output loss becomes:

\begin{align*}
    \frac{\partial L_i}{\partial\hat{y}_{i,j}} = \frac{\partial L_i}{\partial L_{i,j}} \cdot \frac{\partial L_{i,j}}{\partial\hat{y}_{i,j}} = -\frac{1}{J} \cdot \left( \frac{y_{i,j}}{\hat{y}_{i,j}} - \frac{1-y_{i,j}}{1-\hat{y}_{i,j}} \right)
\end{align*}

This normalization has to be performed due to the fact that each output returns its own derivative. Without normalization each additional input will raise gradients and would require changing the hyperparameters like the learning rate.

\section*{Regression}
Aside from \emph{classification}, where it is determined what something is, the other main type of of model is \emph{regression}, which aims to determine a specific value. This requires a model that accepts scalar values, has more granular output, a new way to measure loss, and another output layer activation function.

\subsection*{Activation Function}
Regression uses a linear activation function is one that simply returns the input as the output: $f(x): y=x$. For the backwards pass the derivative becomes $\frac{\diff f(x)}{\diff x} = 1$.

\subsection*{Loss}
The two main methods for calculating error in regression are \emph{mean squared error} (MSE) and \emph{mean absolute error} (MAE). Mean absolute error is actually just the same as L1 error, and mean squared error is like L2 error.

\subsubsection*{Mean Squared Error}
In mean squared error, the difference between the predicted and true values of single outputs are squared and then averaged:

\begin{equation*}
    L_i = \frac{1}{J} \sum_j (y_{i,j} - \hat{y}_{i,j})^2
\end{equation*}

Where $y$ is the target value, $\hat{y}$ means predicted value, $i$ is the index of the current sample, $j$ is the index of the current output in this sample, and $J$ is the number of outputs.

The idea is to penalize more harshly the further away the predicted value is from the intended target. 

Its partial derivative is given as:

\begin{align*}
    \frac{\partial}{\partial\hat{y}_{i,j}} L_i &= \frac{\partial}{\partial\hat{y}_{i,j}} \left[ \frac{1}{J}\sum_j(y_{i,j} - \hat{y}_{i,j})^2 \right] \\
    &= \frac{1}{J} \cdot \frac{\partial}{\partial\hat{y}_{i,j}}(y_{i,j} - \hat{y}_{i,j})^2 \\
    &= \frac{2}{J} \cdot (y_{i,j} - \hat{y}_{i,j}) \cdot \frac{\partial}{\partial\hat{y}_{i,j}}[y_{i,j}-\hat{y}_{i,j}] \\
    &= \frac{2}{J} \cdot (y_{i,j} - \hat{y}_{i,j}) \cdot (\frac{\partial}{\partial\hat{y}_{i,j}}y_{i,j} - \frac{\partial}{\partial\hat{y}_{i,j}}\hat{y}_{i,j}) \\
    &= \frac{2}{J} \cdot (y_{i,j} - \hat{y}_{i,j}) \cdot (0-1) \\
    &= -\frac{2}{J} \cdot (y_{i,j} - \hat{y}_{i,j})
\end{align*}

\subsubsection*{Mean Absolute Error}
In mean absolute error the absolute difference between the predicted and true values in a single output is computed and then averaged over all the outputs:

\begin{equation*}
    L_i = \frac{i}{J}\sum_j\abs{y_{i,j}-\hat{y}_{i,j}}
\end{equation*}

Where $y$ is the target value, $\hat{y}$ is the predicted value, $i$ is the index of the current sample, $j$ is the index of the current output in this sample, and $J$ is the number of outputs.

Mean absolute error, when used as a loss function, penalizes the error linearly. It produces sparser results than MSE and is robust to outliers, which can be both adventageous and disadvantageous. This is why MAE is used less frequently than MSE loss.

The partial derivative for absolute error with respect to the predicted values is:

\begin{align*}
    \frac{\partial}{\partial\hat{y}_{i,j}}L_i &= \frac{\partial}{\partial\hat{y}_{i,j}} \left[ \frac{1}{J}\sum_j\abs{y_{i,j}-\hat{y}_{i,j}} \right] \\
    &= \frac{1}{J} \cdot \frac{\partial}{\partial\hat{y}_{i,j}}\abs{y_{i,j}-\hat{y}_{i,j}} \\
    &= \frac{1}{J} \cdot \begin{cases}
        +1 & y_{i,j} - \hat{y}_{i,j} > 0 \\
        -1 & y_{i,j} - \hat{y}_{i,j} < 0
    \end{cases}
\end{align*}

This function is undefined at 0, but in the implementation it should return 0 when $y_{i,j} - \hat{y}_{i,j} = 0$.

\subsection*{Accuracy}
To measure performance of the model there are to problems. The first is that each output neuron represents a separate output that cannot be combined to calculate accuracy. The second is that the prediction is a scalar value that cannot be directly compared to the ground truth one due to floating-point errors. In actuality, there is no perfect way to show accuracy with regression so we need to simulate or approximate it.

One method is to use a precision value that is calculated using the standard deviation from the ground truth target values and dividing them by some "limit" $\lambda$. The larger this value is, the more strict the accuracy metric will be:

\begin{equation*}
    \rho = \frac{\sigma(y)}{\lambda} = \frac{1}{\lambda} \cdot \sqrt{\frac{\sum_j(y_j-\mu)^2}{J}}
\end{equation*}

Where $j$ is one output, $J$ is the number of outputs, or the size of the population, and $
\mu$ is the \emph{population mean}.

This precision value can be used as an "allowance" for regression outputs when comparing targets and predicted values for accuracy. The comparison is computed by applying the absolute value on the difference between the ground truth and the predictions. Then we check if the difference is smaller than the previously calculated precision:

\begin{equation*}
    \frac{1}{J} \cdot \sum_j^J(\abs{\hat{y}_j-y_j} < \rho)
\end{equation*}

\subsection*{Glorot}
In the backpropagation step, the gradient is calculated using weights, and the learning rate does not effect it. This is why it is important to use the correct weight initialization. In the \emph{Glorot uniform} initialization method, the fraction that multiplies the draw from the uniform distribution is not constant but depends on the number of inputs and the number of neurons. Using this technique to initialize weights can improve the model in cases where the model is not learning at all.


\section*{Data Preprocessing}
The following \emph{preprocessing} steps must be undertaken for a training dataset:

\begin{enumerate}
    \item Balancing
    \item Augmentation (if applicable)
    \item Grayscaling (for images)
    \item Scaling
    \item Flattening
    \item Shuffling
\end{enumerate}

A dataset used for classification is \emph{balanced} if each class occurs with the same frequency. If a dataset is not balanced, a model will likely become biased to predict the class containing the most images. This is because neural networks seek out the steepest and quickest gradient descent to decrease loss, which is most easily found in a local minimum of the most prominent specific class. 

In an imbalanced dataset, it is better to trim away samples from the high-frequency classes so that each class has the same amount of samples. Another option for image datasets is to \emph{augment} the samples by cropping, rotating, or flipping input images, thus creating new samples to balance out the dataset. 

It is common practice to \emph{grayscale} input images and to resize them to normalize their pixel dimensions so they will have the same amount of input values. This results in grayscale values in the $[0,255]$ range for each pixel.

Next, the data needs to be \emph{scaled} into a $[0,1]$ or $[-1,1]$ range. The manner of how to best scale a dataset takes some experimentation and trial and error. If linear scaling does not suffice because of outliers, some combination of the average value and standard deviation may prove useful. Both the training, testing, and prediction data must be scaled using identical methods. However, any preprocessing transformations must be informed only from the training dataset, and not be derived from the testing dataset. 

Since dense layers operate on batches of 1-dimensional vectors, any 2-dimensional features (such as pixels) must be \emph{flattened} into a singular row. Note that some convolutional neural networks will allow passing 2D image arrays as-is.

Another important transformation is \emph{shuffling}: the process of randomizing the order of the input classes. When all classes are clumped together in chunks, the training step will find new local minima for each classification, likely spiking on the final classification in the order and never finding a global minimum. It is important to realize that this preprocessing step is only necessary when training is done in various batches of data, but this is the general use case. 

\subsection*{Batches}
A batch is a slice of a fixed size from a preprocessed dataset. Training with batches means iterating through the dataset in a chunk of data at a time, performing the forward pass, loss calculation, backward pass, and optimization. 

Each batch should be large enough and representative of the dataset, so that the result of training on a batch is a good approximation of the direction towards a global minimum. If the batch size is too small, the direction of gradient descent can fluctuate too much, causing the model to take a long time to train. 

Common batch sizes range between 32 and 128 samples. These can be made smaller in case of memory restraints, or larger to increase the speed (but not necessarily effectiveness) of training. Loss and accuracy improvements can usually be seen by increasing the batch size by a factor of 2 or 3. At some point increasing it will have diminishing returns and will become slow. 

Each batch of samples being trained is referred to as a \emph{step}. The outer loop of the training algorithm iterates over the number of epochs and the inner one over the number of steps. Both the batch-wise and epoch-wise loss and accuracy statistics are tracked. The overall epoch-wise loss and accuracy are sample-wise averages.

\section*{Model Usage}
During model training, the usual \emph{evaluation} process is to analyze its performance, then adjust the hyperparameters, and retrain it using the training and validation data. Then, when the model and hyperparameters that appear to perform the best are found, that model is used on testing data and to make predictions in production. 

Running evaluation on the training data \textit{at the end of the training process} will return the final accuracy and loss. There will be some difference between this mean accuracy and loss and those accumulated during an epoch while the model was still learning.

\subsection*{Model Saving}
A model's state should be able to be saved for future use in \emph{predictions} or as a \emph{checkpoint} during the training process. There are two options that have different use cases: either only the parameters (weights and biases) are saved, or the whole model with all hyperparameters, layers, and optimizer state. 

Saved parameters can be used to initialize a model that has been trained from similar data and then applied to work with specific data. This is called \emph{transfer learning}. Parameters can also be used to visualize the model, to identify dead neurons, or for \emph{reinforcement learning}, where weights collected from multiple models are committed to a single network. While it is possible to train the model further using just parameters, it is more optimal to load a full model including the optimizer's state.

\subsection*{Prediction}
Prediction, or \emph{inference}, is the process of loading a trained model and using it to predict the correct classification of a never-before-seen input such as an image.

The output of a trained classification model is an array of \emph{confidence} vectors that contain a confidence metric per class. The highest confidence score points to the \emph{prediction}. With a softmax classifier the predicted class is simply the $\text{argmax}$ of each confidence vector.

\section*{Conclusion}
The largest challenge of using neural networks is how to best structure a problem so that a model can fit and generalize it, and how to interpret the network's failure.

\end{document}
