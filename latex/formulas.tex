\documentclass[a4paper]{article}
\usepackage[a4paper, left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{amsmath, amssymb, bm, enumitem, graphicx}

\allowdisplaybreaks

\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\euler}{\mathrm{e}}
\newcommand{\mul}{\text{mul}}
\newcommand{\relu}{\text{ReLU}}
\newcommand{\argmax}{\text{argmax}}

\newcommand{\haty}{\hat{y}}

\newcommand{\ELL}{\mathcal{L}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}

\DeclareEmphSequence{\bfseries\slshape}
\setlength\parindent{0pt}
\setlength\parskip{4pt}
\setlist{noitemsep, topsep=4pt, parsep=2pt, partopsep=0pt}

\title{\vspace{-1in}Formulas}
\author{}
\date{}

\begin{document}
\maketitle
\vspace{-0.5in}

\section*{Definitions}

\begin{itemize}
\item $X$: vector of input features, where $x_i$ denotes the $i$-th input feature
\item $y$: true or target output associated with the input data
\item $\hat{y}$: predicted output produced by the neural network
\item $W$: weight matrix with dimension $n \times m$, where $m$ is the number of neurons in the dense layer
\item $b$: bias vector with dimension $m$
\item $z$: weighted sum of inputs plus the bias term (the logits or pre-activations)
\item $h=\sigma(z)$: layer output vector after applying activation function $\sigma$ element-wise over $z$
\end{itemize}

\section*{Forward Propagation}

\begin{equation*}
    \haty = f_L((X \cdot W_1+b_1) \cdot W_2 + b_2 \cdot \ldots \cdot W_L + b_L)
\end{equation*}
Where:
\begin{itemize}
    \item $L$: output layer
    \item $f_L$: activation function applied at the output layer $L$
    \item $X$: vector of input features
    \item $W_l$: weight matrix of layer $l$
    \item $b_l$: bias vector of layer $l$
\end{itemize}

\subsection*{Layers}

\subsubsection*{Dense}
The formula for the operation performed by a single dense layer is a neural network is:
\begin{align*}
    z = W \cdot x + b
\end{align*}
Where:
\begin{itemize}
    \item $z$: weighted sum of inputs plus the bias term
    \item $W$: weight matrix of the dense layer with dimension $n \times m$, where $m$ is the number of neurons in the dense layer
    \item $x$: input vector to the dense layer with dimension $n$
    \item $b$: bias vector of the dense layer with dimension $m$
\end{itemize}

Or in matrix notation:
\begin{equation*}
    z_j = \sum_{i=1}^{n} w_{ij} \cdot x_i + b_j
\end{equation*}
Where:
\begin{itemize}
    \item $z_j$: $j$-th element of the output vector $z$
    \item $n$: number of neurons in the layer
    \item $w_{ij}$: the weight connecting the $i$-th input neuron to the $j$-th neuron in the dense layer
    \item $x_i$: the $i$-th element of input vector $x$
    \item $b_j$: the bias term for the $j$-th neuron in the dense layer
\end{itemize}

\subsubsection*{Dropout}
A binomial experiment consists of a fixed number $n$ of statistically independent Bernoulli trials, each with a probability of success $p$, and counts the number of successes. A random variable corresponding to a binomial experiment denoted by $B(n,p)$ has a binomial distribution. The probability of exactly $k$ successes in experiment $B(n,p)$ is given by:

\begin{equation*}
    P(k) = B(n,p) = \binom{n}{k} p^k (1-p)^{n-k}
\end{equation*}
Where:
\begin{itemize}
    \item $\binom{n}{k}$: binomial coefficient
    \item $n$: number of trials
    \item $k$: number of successes
    \item $p$: probability of success
    \item $(1-p)$: probability of failure
\end{itemize}

The Bernoulli distribution is a special case of the binomial distribution with $n=1$ and $k \in {0,1}$:
\begin{equation*}
    B(1,p) = p^k (1-p)^{1-k}
\end{equation*}
Or equivalently:
\begin{equation*}
    B(1,p) = \begin{cases}
        p   & \text{ if } x=1\\
        1-p & \text{ if } x=0
    \end{cases}
\end{equation*}

Draw a random binary mask from the Bernoulli distribution described above with probability $p$ to generate a sequence of binary values (0s and 1s). Then the formula for the operation of a dropout layer is given as:
\begin{align*}
    z = x \cdot B(1,p) \frac{1}{1-p}
\end{align*}
Where:
\begin{itemize}
    \item $z$: output after dropout
    \item $x$: input vector
    \item $B(1,p)$: Bernoulli distribution with probability $p$
    \item $p$: probability of success
\end{itemize}

When dropout is applied during training, a fraction $p$ of neuron outputs are randomly set to zero. Thus, the remaining fraction $1-p$ of neuron outputs are scaled up by a factor of $\frac{1}{1-p}$ to compensate for the dropped neurons. During inference, dropout is not applied.

\subsubsection*{Recurrent}
Placeholder
\subsubsection*{Convolutional}
Placeholder

\subsection*{Activation Functions}
\subsubsection*{Linear}
\begin{equation*}
    \sigma(x) = x
\end{equation*}
Where:
\begin{itemize}
    \item $\sigma(x)$: linear activation function
    \item $x$: input vector
\end{itemize}

\subsubsection*{Sigmoid}
\begin{equation*}
    \sigma(x) = \frac{1}{1+\euler^{-x}}
\end{equation*}
Where:
\begin{itemize}
    \item $\sigma(x)$: sigmoid activation function
    \item $\euler$: Euler's number, or the base of the natural logarithm
    \item $x$: input vector
\end{itemize}

\subsubsection*{Rectifier}
\begin{equation*}
    \sigma(x) = \max(0,x)
\end{equation*}
Where:
\begin{itemize}
    \item $\sigma(x)$: rectifier or Rectified Linear Unit activation function
    \item $\max(0,x)$: function that returns 0 or $x$ depending on which one is larger
    \item $x$: input vector
\end{itemize}

\subsubsection*{Softmax}
\begin{equation*}
    \sigma_i(z) = \frac{\euler^{z_i}}{\sum_{j=1}^{N} \euler^{z_{ij}}}
\end{equation*}
Where:
\begin{itemize}
    \item $\sigma(z)_i$: probability assigned to the $i$-th class after applying the Softmax function
    \item $z_i$: logit corresponding to the $i$-th class
    \item $\euler$: Euler's number, or the base of the natural logarithm
    \item $N$: total number of classes
\end{itemize}

\subsection*{Loss}
\subsubsection*{Mean Absolute Error}
\begin{equation*}
    \ELL = \frac{1}{n} \sum_{i=1}^{n} \abs{\haty_i - y_i}
\end{equation*}
Where:
\begin{itemize}
    \item $\haty_i$: predicted value for the $i$-th sample
    \item $y_i$: true value for the $i$-th sample
    \item $\abs{\cdot}$: absolute value
\end{itemize}

\subsubsection*{Mean Squared Error}
\begin{equation*}
    \ELL = \frac{1}{n} \sum_{i=1}^{n} (\haty_i - y_i)^2
\end{equation*}
Where:
\begin{itemize}
    \item $\haty_i$: predicted value for the $i$-th sample
    \item $y_i$: true value for the $i$-th sample
\end{itemize}

\subsubsection*{Binary Cross-Entropy}
\begin{equation*}
    \ELL = -\frac{1}{N} \sum_{i=1}^{N} (y_i \, \log(\haty_i) + (1-y_i) \, \log(1-\haty_i))
\end{equation*}
Where:
\begin{itemize}
    \item $N$: number of classes
    \item $y_i$: truth label for the $i$-th sample
    \item $\haty_i$: predicted value (0 or 1) for the $i$-th sample
\end{itemize}


\subsubsection*{Categorical Cross-Entropy}
\begin{equation*}
    \ELL = -\frac{1}{N} \sum_{i=1}^{N} y_i \; \log(\haty_i)
\end{equation*}
Where:
\begin{itemize}
    \item $N$: number of samples
    \item $y_i$: truth label for the $i$-th sample
    \item $\haty_i$: predicted value (0 or 1) for the $i$-th sample
\end{itemize}
When $y$ is expressed as one-hot vectors, the equation simplifies to:
\begin{equation*}
    \ELL = -\frac{1}{N} \log(\haty_{i,k})
\end{equation*}
Where $k$ is the index of the target label.


\newpage
\section*{Backward Propagation}
The backpropagation process involves the following steps:
\begin{enumerate}
    \item Compute the error signals $\delta$ for each layer $l$. An error signal is the gradient of the loss function with respect to the activations $a$ and its formula thus depends on the chosen activation and loss functions.
    
    For the final layer $L$ the error signal is given as:
    \begin{align*}
        \delta_L &= \frac{\partial\ELL}{\partial a_L} \odot \sigma'_L(z_L)\\
                 &= \ELL'_L \odot \sigma'_L(z_L)
    \end{align*}
    Where:
    \begin{itemize}
        \item $\odot$ denotes element-wise multiplication
        \item $\frac{\partial\ELL}{\partial a_L}$ or $\ELL'_L$: partial derivative of the loss function with respect to the activations of layer $L$
        \item $a_L$: activations of layer $L$
        \item $\sigma'_L$: derivative of activation function $\sigma$ of layer $L$
        \item $z_L$: pre-activation matrix or logits of layer $L$ ($z_L = W_L\cdot a_L + b_L$)
    \end{itemize}
    The error signal at hidden layers is computed based on the error signals of the subsequent layers and the weights connecting the current layer to the following ones. The formula involves backpropagating the error signal from the subsequent layer ($\delta_{l+1}$) through the weights, combined with the derivative of the activation function in the current layer $\sigma'_l$:
    \begin{equation*}
        \delta_l = (W_l^T \cdot \delta_{l+1}) \odot \sigma'_l(z_l)
    \end{equation*}
    Where:
    \begin{itemize}
        \item $\cdot$ denotes the dot product
        \item $W_l$: weight matrix of layer $l$
        \item $\sigma'_l$: derivative of activation function $\sigma$ of layer $l$
        \item $z_l$: pre-activation matrix or logits of layer $l$ ($z_l = W_l\cdot a_{l+1} + b_l$)
    \end{itemize}

    \item Compute the parameters (weights and biases) for each layer using the error signal and the activations of the previous layer:
    \begin{align*}
        \nabla W_l = \frac{\partial\ELL}{\partial W_l} &= a_{l+1} \cdot \delta_l \\
        \nabla b_l =\frac{\partial\ELL}{\partial b_l} &= \sum_{i=1}^{n} \delta_{l(ij)} \\
    \end{align*}
    Where $\delta_{l(ij)}$ denotes the error signal (gradient) for the $j$-th neuron in layer $l$ for the $i$-th sample in the batch. The summation sums up all the rows of the error signal matrix along the columns.
    \item Update the parameters using an optimizer such as gradient descent:
    \begin{align*}
        W_l &\leftarrow W_l - \alpha \cdot \nabla W_l\\
        b_l &\leftarrow b_l - \alpha \cdot \nabla b_l
    \end{align*}
    Where $\alpha$ is the learning rate. For another optimizer this equation will be more complex.
\end{enumerate}

% A = derivative of loss function
% B = calculate derivative of activation function of final layer (dvalues = A)
% C = calculate derivative of final layer (dvalues = B)
% D = calculate derivative of activation function of previous layer (dvalues = C)
% E = calculate derivative of previous layer (dvalues = D)
% continue until first layer

\subsection*{Activation}
In the backward step for an activation function $\sigma$ its derivative with respect to the logits $z$ (pre-activation values of a layer) must be calculated and is represented as:
\begin{align*}
    \frac{\partial\ELL}{\partial\sigma} = \sigma'(z)
\end{align*}

\subsubsection*{Linear}
\begin{equation*}
    \sigma'(z) = 1
\end{equation*}
The backward pass for a linear activation function simply involves passing the error signal backward unchanged through the layer.

\subsubsection*{Sigmoid}
\begin{equation*}
    \sigma'(z) = \sigma(z) \cdot (1-\sigma(z))
\end{equation*}

\subsubsection*{Rectified Linear Unit}
\begin{equation*}
    \sigma'(z) = \begin{cases}
        1 & \text{ if } z > 0\\
        0 & \text{ if } z \leq 0
    \end{cases}
\end{equation*}

\subsubsection*{Softmax}
\begin{equation*}
    \sigma'(z) = \frac{\partial \haty_i}{\partial z_j} = \begin{cases}
        \haty_i-\haty_i\haty_j & \text{ when } i  =  j \\
               -\haty_i\haty_j & \text{ when } i\neq j
    \end{cases}
\end{equation*}
Where:
\begin{itemize}
    \item $\haty_i$: $i$-th element of the Softmax function predictions $\haty$
    \item $z_j$: $j$-th element of the Softmax function input vector $z$
\end{itemize}

\subsection*{Loss}
\subsubsection*{Mean Absolute Error}
\begin{equation*}
    \ELL'(\haty_i) = \frac{\partial\ELL}{\partial \haty_i} = \frac{1}{n}\text{sign}(y_i-\haty_i)
\end{equation*}
Where 
\begin{itemize}
    \item $\haty_i$: $i$-th element of the Softmax function predictions $\haty$
    \item $y_i$: $i$-th element of the target label vector $y$
    \item $\text{sign}(x)$: $-1 \text{ if } x < 0, 0 \text{ if } x = 0, \text{ and } 1 \text{ if } x > 0$
\end{itemize}

\subsubsection*{Mean Squared Error}
\begin{equation*}
    \ELL'(\haty_i) = \frac{\partial\ELL}{\partial \haty_i} = -\frac{1}{n}2(\haty_i-y_i)
\end{equation*}
Where:
\begin{itemize}
    \item $\haty_i$: $i$-th element of the Softmax function predictions $\haty$
    \item $y_i$: $i$-th element of the target label vector $y$
\end{itemize}

\subsubsection*{Binary Cross-Entropy}
\begin{equation*}
    \ELL'(\haty_i) = \frac{\partial\ELL}{\partial \haty_i} = -\frac{1}{n}\left[ \frac{y_i}{\haty_i} - \frac{1-y_i}{1-\haty_i} \right]
\end{equation*}
Where:
\begin{itemize}
    \item $\haty_i$: $i$-th element of the Softmax function predictions $\haty$
    \item $y_i$: $i$-th element of the target label vector $y$
\end{itemize}

\subsubsection*{Categorical Cross-Entropy}
\begin{equation*}
    \ELL'(\haty_i) = \frac{\partial\ELL}{\partial \haty_i} = -\frac{1}{n}\frac{y_i}{\haty_i}
\end{equation*}
Where:
\begin{itemize}
    \item $\haty_i$: $i$-th element of the Softmax function predictions $\haty$
    \item $y_i$: $i$-th element of the target label vector $y$
\end{itemize}

\subsubsection*{Softmax Categorical Cross-Entropy}
\begin{equation*}
    \ELL'(z_i) = \frac{\partial\ELL}{\partial z_i} = \haty_i - y_i
\end{equation*}
Where:
\begin{itemize}
    \item $z_i$: $i$-th element of the Softmax function input vector $z$
    \item $\haty_i$: $i$-th element of the Softmax function predictions $\haty$
    \item $y_i$: $i$-th element of the target label vector $y$
\end{itemize}

\pagebreak
\section*{Optimization}

\subsection*{Stochastic Gradient Descent}
\begin{equation*}
    \theta_{t+1} = \theta_t - \eta \nabla\ELL(\theta_t)
\end{equation*}
Where:
\begin{itemize}
    \item $\theta_t$: value of the parameter at time step $t$
    \item $\eta$: learning rate
    \item $\nabla\ELL(\theta_t)$: gradient of the loss function $\ELL$ with respect to parameter $\theta$ at time step $t$
\end{itemize}

\subsection*{Momentum}
\begin{align*}
    v_{t+1}      &= \gamma \cdot v_t + (1-\gamma) \nabla\ELL(\theta_t) \\
    \theta_{t+1} &= \theta_t - \eta \cdot v_{t+1}
\end{align*}
Where:
\begin{itemize}
    \item $\theta_t$: value of the parameter at time step $t$
    \item $\eta$: learning rate
    \item $\nabla\ELL(\theta_t)$: gradient of the loss function $\ELL$ with respect to parameter $\theta$ at time step $t$
    \item $v_t$: moving average of the gradients at time step $t$
    \item $\gamma$: momentum hyperparameter (value between 0 and 1)
\end{itemize}

\subsection*{AdaGrad}
\begin{align*}
    v_{t+1} &= v_t + \nabla\ELL(\theta_t)^2 \\
    \theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{v_{t+1}}+\epsilon} \nabla\ELL(\theta_t)
\end{align*}
Where:
\begin{itemize}
    \item $\theta_t$: value of the parameter at time step $t$
    \item $\eta$: learning rate
    \item $\nabla\ELL(\theta_t)$: gradient of the loss function $\ELL$ with respect to parameter $\theta$ at time step $t$
    \item $v_t$: sum of squares of past gradients up to time step $t$
    \item $\epsilon$: small constant (usually on the order of $10^8$) to avoid division by zero
\end{itemize}

\subsection*{AdaDelta}
\begin{align*}
    v_{t+1} &= \rho v_t + (1-\rho)\nabla\ELL(\theta_t)^2 \\
    \Delta\theta_{t+1} &= -\frac{\sqrt{\Delta\theta_t}+\epsilon}{\sqrt{v_{t+1}}+\epsilon} \nabla\ELL(\theta_t) \\
    \theta_{t+1} &= \theta_t + \Delta\theta_{t+1}
\end{align*}
Where:
\begin{itemize}
    \item $\theta_t$: value of the parameter at time step $t$
    \item $\nabla\ELL(\theta_t)$: gradient of the loss function $\ELL$ with respect to parameter $\theta$ at time step $t$
    \item $v_t$: exponentially decaying moving average of squared gradients at time step $t$
    \item $\rho$: decay rate for the moving average, typically close to $1$ (e.g. $0.95$)
    \item $\epsilon$: small constant to avoid division by zero
    \item $\Delta\theta_{t+1}$: update term for the parameter at time step $t+1$, computed based on the root mean square of the parameter updates
    \item $\Delta\theta_{t}$: exponentially decaying moving average of squared parameter updates at time step $t$
\end{itemize}

\subsection*{RMSProp}
\begin{align*}
    v_{t+1} &= \rho \, v_{t} + (1-\rho) {\nabla\ELL(\theta_t)}^2 \\
    \theta_{t+1} &= \theta_{t} - \frac{\eta}{\sqrt{v_{t+1}} + \epsilon} \nabla\ELL(\theta_t)
\end{align*}
Where:
\begin{itemize}
    \item $\theta_t$: value of the parameter at time step $t$
    \item $\eta$: learning rate
    \item $\nabla\ELL(\theta_t)$: gradient of the loss function $\ELL$ with respect to parameter $\theta$ at time step $t$
    \item $v_t$: exponentially decaying moving average of squared gradients at time step $t$
    \item $\rho$: decay rate, typically set to a value close to $1$ (e.g. $0.9$)
    \item $\epsilon$: small constant to avoid division by zero
\end{itemize}

\subsection*{Adam}
\begin{align*}
    m_{t+1} &= \beta_1 m_t + (1-\beta_1) \nabla\ELL(\theta_t) \\
    v_{t+1} &= \beta_2 v_t + (1-\beta_2) \nabla\ELL(\theta_t)^2\\
    \hat{m}_{t+1} &= \frac{m_{t+1}}{1-\beta_1^t}\\
    \hat{v}_{t+1} &= \frac{v_{t+1}}{1-\beta_2^t}\\
    \theta_{t+1} &= \theta_t - \frac{\eta}{\sqrt{\hat{v}_{t+1}}+\epsilon} \hat{m}_{t+1}
\end{align*}
Where:
\begin{itemize}
    \item $\theta_t$: value of the parameter at time step $t$
    \item $\eta$: learning rate
    \item $\nabla\ELL(\theta_t)$: gradient of the loss function $\ELL$ with respect to parameter $\theta$ at time step $t$
    \item $m_t$ and $v_t$ are the first and second moments (moving averages) of the gradients at time step $t$
    \item $\hat{m}_{t+1}$ and $\hat{v}_{t+1}$ are bias-corrected estimates of the first and second moments
    \item $\beta_1$ and $\beta_2$ are the decay rates for the moving averages, typically close to $1$ (e.g. $0.9$ and $0.999$, respectively)
    \item $\epsilon$: small constant to avoid division by zero
\end{itemize}

\end{document}
